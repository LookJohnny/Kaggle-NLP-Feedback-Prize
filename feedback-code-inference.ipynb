{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#https://www.kaggle.com/competitions/feedback-prize-effectiveness\n#https://www.kaggle.com/datasets/hiromoon166/deberta-v3-large\n#https://www.kaggle.com/datasets/kpriyanshu256/debertalarge\n#训练完成权重","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 句子分类\ndeberta_v3_large_v3.py训练保存的权重","metadata":{}},{"cell_type":"code","source":"%%writefile deberta_v3.py\n# import manipulation\nimport numpy as np\nimport pandas as pd\n\n# import Pytorch\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.checkpoint import checkpoint\nfrom torch.autograd import Variable\n\n# import Transformer model\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, AdamW\nfrom transformers import DataCollatorWithPadding\nfrom transformers.models.deberta_v2.modeling_deberta_v2 import StableDropout, ContextPooler\n\n# import SKLearn\nfrom sklearn.model_selection import  KFold, GroupKFold, StratifiedKFold, StratifiedGroupKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import log_loss\n\n\n# import ...\nimport string\nimport random\nimport os\nimport joblib\nimport gc\nimport copy\nimport time\n\n\n# other\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ntransformers.logging.set_verbosity_error()\nclass CFG:\n    seed = 2022\n    max_length = 512\n    epoch = 4\n    train_batch_size = 8\n    valid_batch_size = 8\n    model_name = \"../input/deberta-v3-large/deberta-v3-large\"\n    token_name = \"../input/deberta-v3-large/deberta-v3-large\"\n    num_classes = 3\n    n_fold = 5\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.token_name, use_fast=True)\nCFG.tokenizer.model_max_length = CFG.max_length\nCFG.tokenizer.is_fast\ndef softmax(z):\n    assert len(z.shape) == 2\n    s = np.max(z, axis=1)\n    s = s[:, np.newaxis] # necessary step to do broadcasting\n    e_x = np.exp(z - s)\n    div = np.sum(e_x, axis=1)\n    div = div[:, np.newaxis] # dito\n    return e_x / div\nclass FeedbackDataset(Dataset):\n    def __init__(self,df, max_length, tokenizer, training=True):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.discourse_type = self.df['discourse_type'].values\n        self.discourse_text = self.df['discourse_text'].values\n        self.essays = self.df['essay_text'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        discourse_type = self.discourse_type[index]\n        discourse_text = self.discourse_text[index]\n        essay = self.essays[index]\n        type_text = discourse_type + ' ' + discourse_text\n        \n        inputs = self.tokenizer.encode_plus(\n            type_text, \n            essay,\n            truncation = True,\n            add_special_tokens = True,\n            return_token_type_ids = True,\n            max_length = self.max_len\n        )\n        \n        samples = {\n            'input_ids': inputs['input_ids'],\n            'attention_mask': inputs['attention_mask'],\n        }\n        return samples\n\n\n# Dynamic Padding (Collate)\nclass Collate:\n    def __init__(self, tokenizer, isTrain=True):\n        self.tokenizer = tokenizer\n        self.isTrain = isTrain\n        # self.args = args\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n        if self.isTrain:\n            output[\"target\"] = [sample[\"target\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n            output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n        else:\n            output[\"input_ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"input_ids\"]]\n            output[\"attention_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"attention_mask\"]]\n\n        # convert to tensors\n        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n        if self.isTrain:\n            output[\"target\"] = torch.tensor(output[\"target\"], dtype=torch.long)\n\n        return output\n\n# collate_fn = DataCollatorWithPadding(tokenizer=CFG.tokenizer)\n\n\nclass FeedbackModel(nn.Module):\n    def __init__(self, model_name):\n        super(FeedbackModel, self).__init__()\n        \n        # DeBERTa\n        \n        self.config = AutoConfig.from_pretrained(model_name)\n        self.config.update({\"output_hidden_states\":True, \n                      'return_dict':True}) \n        self.backbone = AutoModel.from_pretrained(model_name, config=self.config)\n        self.fc = nn.Linear(self.config.hidden_size, CFG.num_classes)\n\n    def forward(self, ids, mask):        \n        out = self.backbone(input_ids=ids,attention_mask=mask)\n        hs = out['hidden_states']\n        x = hs[-1][:, 0, :]\n        x = self.fc(x)\n\n        return x\n\ntest_df = pd.read_csv(\"../input/feedback-prize-effectiveness/test.csv\")\ntest_df.head()\n\nINPUT_DIR = \"/kaggle/input/feedback-prize-effectiveness\"\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nTEST_CSV = os.path.join(INPUT_DIR, \"test.csv\")\n\ndef get_essay_test(essay_id):\n    path = os.path.join(TEST_DIR, f'{essay_id}.txt')\n    essay_text = open(path, 'r').read()\n    return essay_text\n\ntest_df = pd.read_csv(TEST_CSV)\ntest_df['essay_text']= test_df['essay_id'].apply(get_essay_test)\ntest_df['text'] = test_df['essay_text']+' '+test_df['discourse_text']\ntest_df['length'] = test_df['text'].map(len)\ntest_df = test_df.sort_values(['length']).reset_index(drop=True)\ndel test_df['length'], test_df['text']\ngc.collect()\ncollate_fn = Collate(tokenizer=CFG.tokenizer, isTrain=False)\n\n\n\ndef prepare_test_loader(test_df):    \n    test_dataset = FeedbackDataset(test_df, \n                                   tokenizer=CFG.tokenizer, \n                                   max_length=CFG.max_length,\n                                    training=False)\n    \n    test_loader = DataLoader(test_dataset, \n                             batch_size=CFG.valid_batch_size, \n                             collate_fn=collate_fn, \n                             num_workers=2, \n                             shuffle=False, \n                             pin_memory=True, \n                             drop_last=False)\n    return test_loader\n\ntest_loader = prepare_test_loader(test_df)\n\n@torch.no_grad()\ndef inference(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    \n    bar = tqdm(enumerate(test_loader), total=len(test_loader))\n    \n    for step, data in bar: \n        ids = data['input_ids'].to(device, dtype = torch.long)\n        mask = data['attention_mask'].to(device, dtype = torch.long)\n        \n        output = model(ids, mask)\n        y_preds = softmax(output.to('cpu').numpy())\n        \n        preds.append(y_preds)\n         \n    predictions = np.concatenate(preds)\n    return predictions\n\ndeberta_predictions = []\n\nfor fold in range(CFG.n_fold):\n    print(\"Fold {}\".format(fold))\n\n    model = FeedbackModel(CFG.model_name)\n    state = torch.load(f'../input/feedback-prize-effectiveness-v2-models/deberta-V3-large-v3_f{fold}.bin')\n\n    model.load_state_dict(state)\n\n    prediction = inference(test_loader, model, CFG.device)\n    deberta_predictions.append(prediction)\n    del model, state, prediction\n    gc.collect()\n    torch.cuda.empty_cache()\n\npredictions = np.mean(deberta_predictions, axis=0)\npredictions\n\nINPUT_DIR = \"../input/feedback-prize-effectiveness/\"\nsubmission = pd.read_csv(os.path.join(INPUT_DIR, 'sample_submission.csv'))\nsubmission['discourse_id'] = test_df['discourse_id']\nsubmission['Ineffective'] = predictions[:, 0]\nsubmission['Adequate'] = predictions[:, 1]\nsubmission['Effective'] = predictions[:, 2]\n\nsubmission\n\nsubmission.to_csv('sub1.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-19T16:50:27.181805Z","iopub.execute_input":"2022-08-19T16:50:27.182277Z","iopub.status.idle":"2022-08-19T16:50:27.223039Z","shell.execute_reply.started":"2022-08-19T16:50:27.182187Z","shell.execute_reply":"2022-08-19T16:50:27.221854Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!python deberta_v3.py","metadata":{"execution":{"iopub.status.busy":"2022-08-19T16:50:28.240293Z","iopub.execute_input":"2022-08-19T16:50:28.241390Z","iopub.status.idle":"2022-08-19T16:53:40.312801Z","shell.execute_reply.started":"2022-08-19T16:50:28.241341Z","shell.execute_reply":"2022-08-19T16:53:40.311588Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### 句子分类\ndeberta_large_v3训练保存的权重","metadata":{}},{"cell_type":"code","source":"%%writefile deberta.py\n# import manipulation\nimport numpy as np\nimport pandas as pd\n\n# import Pytorch\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.checkpoint import checkpoint\nfrom torch.autograd import Variable\n\n# import Transformer model\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, AdamW\nfrom transformers import DataCollatorWithPadding\nfrom transformers.models.deberta_v2.modeling_deberta_v2 import StableDropout, ContextPooler\n\n# import SKLearn\nfrom sklearn.model_selection import  KFold, GroupKFold, StratifiedKFold, StratifiedGroupKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import log_loss\n\n\n# import ...\nimport string\nimport random\nimport os\nimport joblib\nimport gc\nimport copy\nimport time\n\n\n# other\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ntransformers.logging.set_verbosity_error()\nclass CFG:\n    seed = 2022\n    max_length = 512\n    epoch = 4\n    train_batch_size = 8\n    valid_batch_size = 8\n    model_name = \"../input/debertalarge\"\n    token_name = \"../input/debertalarge\"\n    num_classes = 3\n    n_fold = 5\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    \nCFG.tokenizer = AutoTokenizer.from_pretrained(CFG.token_name, use_fast=True)\nCFG.tokenizer.model_max_length = CFG.max_length\nCFG.tokenizer.is_fast\ndef softmax(z):\n    assert len(z.shape) == 2\n    s = np.max(z, axis=1)\n    s = s[:, np.newaxis] # necessary step to do broadcasting\n    e_x = np.exp(z - s)\n    div = np.sum(e_x, axis=1)\n    div = div[:, np.newaxis] # dito\n    return e_x / div\nclass FeedbackDataset(Dataset):\n    def __init__(self,df, max_length, tokenizer, training=True):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.discourse_type = self.df['discourse_type'].values\n        self.discourse_text = self.df['discourse_text'].values\n        self.essays = self.df['essay_text'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        discourse_type = self.discourse_type[index]\n        discourse_text = self.discourse_text[index]\n        essay = self.essays[index]\n        type_text = discourse_type + ' ' + discourse_text\n        \n        inputs = self.tokenizer.encode_plus(\n            type_text, \n            essay,\n            truncation = True,\n            add_special_tokens = True,\n            return_token_type_ids = True,\n            max_length = self.max_len\n        )\n        \n        samples = {\n            'input_ids': inputs['input_ids'],\n            'attention_mask': inputs['attention_mask'],\n        }\n        return samples\n\n\n# Dynamic Padding (Collate)\nclass Collate:\n    def __init__(self, tokenizer, isTrain=True):\n        self.tokenizer = tokenizer\n        self.isTrain = isTrain\n        # self.args = args\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n        if self.isTrain:\n            output[\"target\"] = [sample[\"target\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n            output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n        else:\n            output[\"input_ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"input_ids\"]]\n            output[\"attention_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"attention_mask\"]]\n\n        # convert to tensors\n        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n        if self.isTrain:\n            output[\"target\"] = torch.tensor(output[\"target\"], dtype=torch.long)\n\n        return output\n\n# collate_fn = DataCollatorWithPadding(tokenizer=CFG.tokenizer)\n\n\nclass FeedbackModel(nn.Module):\n    def __init__(self, model_name):\n        super(FeedbackModel, self).__init__()\n        \n        # DeBERTa\n        \n        self.config = AutoConfig.from_pretrained(model_name)\n        self.config.update({\"output_hidden_states\":True, \n                      'return_dict':True}) \n        self.deberta = AutoModel.from_pretrained(model_name, config=self.config)\n        self.fc = nn.Linear(self.config.hidden_size, CFG.num_classes)\n\n    def forward(self, ids, mask):        \n        out = self.deberta(input_ids=ids,attention_mask=mask)\n        hs = out['hidden_states']\n        x = hs[-1][:, 0, :]\n        x = self.fc(x)\n\n        return x\n\ntest_df = pd.read_csv(\"../input/feedback-prize-effectiveness/test.csv\")\ntest_df.head()\n\nINPUT_DIR = \"/kaggle/input/feedback-prize-effectiveness\"\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nTEST_CSV = os.path.join(INPUT_DIR, \"test.csv\")\n\ndef get_essay_test(essay_id):\n    path = os.path.join(TEST_DIR, f'{essay_id}.txt')\n    essay_text = open(path, 'r').read()\n    return essay_text\n\ntest_df = pd.read_csv(TEST_CSV)\ntest_df['essay_text']= test_df['essay_id'].apply(get_essay_test)\ntest_df['text'] = test_df['essay_text']+' '+test_df['discourse_text']\ntest_df['length'] = test_df['text'].map(len)\ntest_df = test_df.sort_values(['length']).reset_index(drop=True)\ndel test_df['length'], test_df['text']\ngc.collect()\ncollate_fn = Collate(tokenizer=CFG.tokenizer, isTrain=False)\n\ndef prepare_test_loader(test_df):    \n    test_dataset = FeedbackDataset(test_df, \n                                   tokenizer=CFG.tokenizer, \n                                   max_length=CFG.max_length,\n                                    training=False)\n    \n    test_loader = DataLoader(test_dataset, \n                             batch_size=CFG.valid_batch_size, \n                             collate_fn=collate_fn, \n                             num_workers=2, \n                             shuffle=False, \n                             pin_memory=True, \n                             drop_last=False)\n    return test_loader\n\ntest_loader = prepare_test_loader(test_df)\n\n@torch.no_grad()\ndef inference(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    \n    bar = tqdm(enumerate(test_loader), total=len(test_loader))\n    \n    for step, data in bar: \n        ids = data['input_ids'].to(device, dtype = torch.long)\n        mask = data['attention_mask'].to(device, dtype = torch.long)\n        \n        output = model(ids, mask)\n        y_preds = softmax(output.to('cpu').numpy())\n        \n        preds.append(y_preds)\n         \n    predictions = np.concatenate(preds)\n    return predictions\n\ndeberta_predictions = []\n\nfor fold in range(CFG.n_fold):\n    print(\"Fold {}\".format(fold))\n\n    model = FeedbackModel(CFG.model_name)\n    state = torch.load(f'../input/feedback-prize-effectiveness-v2-models/deberta-large-v3_f{fold}.bin')\n\n    model.load_state_dict(state)\n\n    prediction = inference(test_loader, model, CFG.device)\n    deberta_predictions.append(prediction)\n    del model, state, prediction\n    gc.collect()\n    torch.cuda.empty_cache()\n\npredictions = np.mean(deberta_predictions, axis=0)\npredictions\n\nINPUT_DIR = \"../input/feedback-prize-effectiveness/\"\nsubmission = pd.read_csv(os.path.join(INPUT_DIR, 'sample_submission.csv'))\nsubmission['discourse_id'] = test_df['discourse_id']\nsubmission['Ineffective'] = predictions[:, 0]\nsubmission['Adequate'] = predictions[:, 1]\nsubmission['Effective'] = predictions[:, 2]\n\nsubmission\n\nsubmission.to_csv('sub2.csv', index=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-19T16:53:40.316310Z","iopub.execute_input":"2022-08-19T16:53:40.316682Z","iopub.status.idle":"2022-08-19T16:53:40.328473Z","shell.execute_reply.started":"2022-08-19T16:53:40.316646Z","shell.execute_reply":"2022-08-19T16:53:40.327436Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!python deberta.py","metadata":{"execution":{"iopub.status.busy":"2022-08-19T16:53:40.329526Z","iopub.execute_input":"2022-08-19T16:53:40.329917Z","iopub.status.idle":"2022-08-19T16:56:34.697229Z","shell.execute_reply.started":"2022-08-19T16:53:40.329881Z","shell.execute_reply":"2022-08-19T16:56:34.695963Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### token分类\n模型配置文件和权重文件都是由deberta_large_train_on_oldweights.py训练生成保存的","metadata":{}},{"cell_type":"code","source":"%%writefile token1.py\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nimport gc\nimport random\nimport warnings\nimport torch\nfrom transformers import Trainer, TrainingArguments, AutoModelForTokenClassification, DataCollatorForTokenClassification, AutoTokenizer, AutoConfig\nfrom itertools import chain\nfrom text_unidecode import unidecode\nfrom typing import Tuple\nimport codecs\nimport re\nfrom functools import partial\nimport datasets\n\n\nwarnings.filterwarnings(\"ignore\")\ngc.collect()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nINPUT_DIR = \"../input/feedback-prize-effectiveness/\"\nmodel_path = '../input/feedback-deberta-large-token-cls-bs4/'\n\nclass CFG:\n    model = \"deberta-large\"\n    max_len = 2048\n    batch_size = 2\n    epochs = 4\n    n_fold = 5\n    trn_fold = [0, 1, 2, 3, 4,]\n    lr = 1e-5\n    weight_decay = 1e-2\n\ndef replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n\ndef replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n\n# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\ncodecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\ncodecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n\ndef resolve_encodings_and_normalize(text: str) -> str:\n    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n    text = (\n        text.encode(\"raw_unicode_escape\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n    )\n    text = unidecode(text)\n    return text\n\n\ndef get_essay_text(sample, data_dir):\n    id_ = sample[\"essay_id\"]\n    with open(data_dir + \"test/\" + f\"{id_}.txt\", \"r\") as fp:\n        sample[\"essay_text\"] = resolve_encodings_and_normalize(fp.read())\n    return sample\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\ndisc_types = [\n    \"Claim\",\n    \"Concluding Statement\",\n    \"Counterclaim\",\n    \"Evidence\",\n    \"Lead\",\n    \"Position\",\n    \"Rebuttal\",\n]\ncls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\nend_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n\nlabel2id = {\n    \"Adequate\": 0,\n    \"Effective\": 1,\n    \"Ineffective\": 2,\n}\n\ncls_id_map = {\n    label: tokenizer.encode(tkn)[1] for label, tkn in cls_tokens_map.items()\n}\n\nid_cls_map = {v: k for k, v in cls_id_map.items()}\n\ndef find_positions(sample):\n    text = sample[\"essay_text\"][0]\n\n    # keeps track of what has already\n    # been located\n    min_idx = 0\n\n    # stores start and end indexes of discourse_texts\n    idxs = []\n\n    for dt in sample[\"discourse_text\"]:\n        # calling strip is essential\n        matches = list(re.finditer(re.escape(dt.strip()), text))\n\n        # If there are multiple matches, take the first one\n        # that is past the previous discourse texts.\n        if len(matches) > 1:\n            for m in matches:\n                if m.start() >= min_idx:\n                    break\n        # If no matches are found\n        elif len(matches) == 0:\n            idxs.append([-1])  # will filter out later\n            continue\n            # If one match is found\n        else:\n            m = matches[0]\n\n        idxs.append([m.start(), m.end()])\n\n        min_idx = m.start()\n\n    return idxs\n\ndef tokenize(sample):\n    sample[\"idxs\"] = find_positions(sample)\n\n    text = sample[\"essay_text\"][0]\n    chunks = []\n    prev = 0\n\n    zipped = zip(\n        sample[\"idxs\"],\n        sample[\"discourse_type\"],\n    )\n    for idxs, disc_type in zipped:\n        # when the discourse_text wasn't found\n        if idxs == [-1]:\n            continue\n\n        s, e = idxs\n\n        # if the start of the current discourse_text is not\n        # at the end of the previous one.\n        # (text in between discourse_texts)\n        if s != prev:\n            chunks.append(text[prev:s])\n            prev = s\n\n        # if the start of the current discourse_text is\n        # the same as the end of the previous discourse_text\n        if s == prev:\n            chunks.append(cls_tokens_map[disc_type])\n            chunks.append(text[s:e])\n            chunks.append(end_tokens_map[disc_type])\n\n        prev = e\n\n    tokenized = tokenizer(\n        \" \".join(chunks),\n        padding=False,\n        truncation=True,\n        max_length=CFG.max_len,\n        add_special_tokens=True,\n    )\n\n    return tokenized\n\ntest_df = pd.read_csv(INPUT_DIR + \"test.csv\")\n\nessay_text_ds = datasets.Dataset.from_dict({\"essay_id\": test_df.essay_id.unique()})\nessay_text_ds = essay_text_ds.map(\n        partial(get_essay_text, data_dir=INPUT_DIR),\n        num_proc=1,\n        batched=False,\n        desc=\"Loading text files\",\n)\nessay_text_df = essay_text_ds.to_pandas()\n\ntest_df[\"discourse_text\"] = [resolve_encodings_and_normalize(x) for x in test_df[\"discourse_text\"]]\ntest_df = test_df.merge(essay_text_df, on=\"essay_id\", how=\"left\")\ndel essay_text_df\n\n# track the matchings of each discourse in its essay, by the order given in the csv file\n\ndiscourse_text_values = test_df['discourse_text'].values\nessay_text_values = test_df['essay_text'].values\n\nmatches = []\nfor i, dt in enumerate(discourse_text_values):\n    if dt.strip() in essay_text_values[i]:\n        matches.append(1)\n    else:\n        matches.append(0)\ntest_df['match'] = matches\n\ngrouped_df = test_df.groupby([\"essay_id\"]).agg(list)\n\nds = datasets.Dataset.from_pandas(grouped_df)\nds = ds.map(\n        tokenize,\n        batched=False,\n        num_proc=1,\n        desc=\"Tokenizing\",\n)\n\nbad_matches = []\ncls_ids = set(list(cls_id_map.values()))\nfor id_, ids, dt in zip(ds[\"essay_id\"], ds[\"input_ids\"], ds[\"discourse_id\"]):\n    # count number of cls ids\n    num_cls_id = sum([x in cls_ids for x in ids])\n    # true number of discourse_texts\n    num_dt = len(dt)\n\n    if num_cls_id != num_dt:\n        bad_matches.append((id_, ids, dt))\n\nprint(\"Num bad matches:\", len(bad_matches))\nprint()\n\ncollator_fn = DataCollatorForTokenClassification(\n    tokenizer=tokenizer, pad_to_multiple_of=8, padding=True\n)\n\nmodel_config = AutoConfig.from_pretrained(model_path)\n\npreds = []\n\nfor fold in range(CFG.n_fold):\n    if fold in CFG.trn_fold:\n        print(f\"fold = {fold}\")\n        \n        checkpoint = f'../input/feedback-deberta-large-token-cls-bs4/pytorch_model_f{fold}.bin'\n        model = AutoModelForTokenClassification.from_pretrained(checkpoint, config=model_config)\n        \n        keep_cols = {\"input_ids\", \"attention_mask\"}\n        test_dataset = ds.remove_columns([c for c in ds.column_names if c not in keep_cols])\n        \n        trainer = Trainer(\n            model=model,\n            tokenizer=tokenizer,\n            data_collator=collator_fn,\n        )\n        \n        pred = trainer.predict(test_dataset)\n        preds.append(pred[0])\n        \n        del model\n        gc.collect()\n        torch.cuda.empty_cache()\n\n# get predicted logits\n\npreds = np.array(preds)\nlogits = np.exp(preds) / np.expand_dims(np.sum(np.exp(preds), axis=-1), axis=-1)\n\nmodel_preds = np.mean(logits, axis=0)\n\n# extract the predicted labels for all [discourse_type_CLS] \n\nhead_preds = []\nfor i, sample in enumerate(model_preds):\n    sample_pred = []\n    sample_ids = ds['input_ids'][i]\n    for j, tk_id in enumerate(sample_ids):\n        if tk_id in cls_ids:\n            sample_pred.append(sample[j])\n    head_preds.append(sample_pred)\n\n# collect predictions of each discourse by the order given in the csv file\n\nessay_id_map = {v : k for k, v in enumerate(ds['essay_id'])}\n\nfinal_preds = []\n\nordered_essay_ids = test_df['essay_id'].values\ndisordered_essay_matches = grouped_df['match'].values\n\npre_essay_id = ''\nfor essay_id in ordered_essay_ids:\n    if essay_id == pre_essay_id:\n        continue\n    pre_essay_id = essay_id\n    essay_pred = head_preds[essay_id_map[essay_id]]\n    essay_macth = disordered_essay_matches[essay_id_map[essay_id]]\n    for i, discourse_match in enumerate(essay_macth):\n        if discourse_match == 1 and i < len(essay_pred):\n            final_preds.append(essay_pred[i])\n        else:\n            final_preds.append([0., 0., 0.])\n            \nfinal_preds = np.array(final_preds)\n\npreds_Ineffective = final_preds[:, 2]\npreds_Adequate = final_preds[:, 0]\npreds_Effective = final_preds[:, 1]\n\nsample = pd.read_csv(INPUT_DIR + 'sample_submission.csv')\n\nsample['Ineffective'] = preds_Ineffective\nsample['Adequate'] = preds_Adequate\nsample['Effective'] = preds_Effective\n\n\nsample.to_csv('sub3.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-19T16:56:34.701559Z","iopub.execute_input":"2022-08-19T16:56:34.701881Z","iopub.status.idle":"2022-08-19T16:56:34.714747Z","shell.execute_reply.started":"2022-08-19T16:56:34.701847Z","shell.execute_reply":"2022-08-19T16:56:34.713624Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!python token1.py","metadata":{"execution":{"iopub.status.busy":"2022-08-19T16:56:45.017691Z","iopub.execute_input":"2022-08-19T16:56:45.018112Z","iopub.status.idle":"2022-08-19T16:58:45.605029Z","shell.execute_reply.started":"2022-08-19T16:56:45.018069Z","shell.execute_reply":"2022-08-19T16:58:45.603774Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### token分类\n模型配置文件和权重文件都是由deberta_v3_large_train_on_oldweights.py训练生成保存的","metadata":{}},{"cell_type":"code","source":"%%writefile token2.py\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nimport gc\nimport random\nimport warnings\nimport torch\nfrom transformers import Trainer, TrainingArguments, AutoModelForTokenClassification, DataCollatorForTokenClassification, AutoTokenizer, AutoConfig\nfrom itertools import chain\nfrom text_unidecode import unidecode\nfrom typing import Tuple\nimport codecs\nimport re\nfrom functools import partial\nimport datasets\n\n\nwarnings.filterwarnings(\"ignore\")\ngc.collect()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nINPUT_DIR = \"../input/feedback-prize-effectiveness/\"\nmodel_path = '../input/feedback-debertav3large-token-cls-models/'\n\nclass CFG:\n    model = \"deberta-large\"\n    max_len = 2048\n    batch_size = 2\n    epochs = 4\n    n_fold = 5\n    trn_fold = [0, 1, 2, 3, 4,]\n    lr = 1e-5\n    weight_decay = 1e-2\n\ndef replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n\ndef replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n\n# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\ncodecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\ncodecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n\ndef resolve_encodings_and_normalize(text: str) -> str:\n    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n    text = (\n        text.encode(\"raw_unicode_escape\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n    )\n    text = unidecode(text)\n    return text\n\n\ndef get_essay_text(sample, data_dir):\n    id_ = sample[\"essay_id\"]\n    with open(data_dir + \"test/\" + f\"{id_}.txt\", \"r\") as fp:\n        sample[\"essay_text\"] = resolve_encodings_and_normalize(fp.read())\n    return sample\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\ndisc_types = [\n    \"Claim\",\n    \"Concluding Statement\",\n    \"Counterclaim\",\n    \"Evidence\",\n    \"Lead\",\n    \"Position\",\n    \"Rebuttal\",\n]\ncls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\nend_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n\nlabel2id = {\n    \"Adequate\": 0,\n    \"Effective\": 1,\n    \"Ineffective\": 2,\n}\n\ncls_id_map = {\n    label: tokenizer.encode(tkn)[1] for label, tkn in cls_tokens_map.items()\n}\n\nid_cls_map = {v: k for k, v in cls_id_map.items()}\n\ndef find_positions(sample):\n    text = sample[\"essay_text\"][0]\n\n    # keeps track of what has already\n    # been located\n    min_idx = 0\n\n    # stores start and end indexes of discourse_texts\n    idxs = []\n\n    for dt in sample[\"discourse_text\"]:\n        # calling strip is essential\n        matches = list(re.finditer(re.escape(dt.strip()), text))\n\n        # If there are multiple matches, take the first one\n        # that is past the previous discourse texts.\n        if len(matches) > 1:\n            for m in matches:\n                if m.start() >= min_idx:\n                    break\n        # If no matches are found\n        elif len(matches) == 0:\n            idxs.append([-1])  # will filter out later\n            continue\n            # If one match is found\n        else:\n            m = matches[0]\n\n        idxs.append([m.start(), m.end()])\n\n        min_idx = m.start()\n\n    return idxs\n\ndef tokenize(sample):\n    sample[\"idxs\"] = find_positions(sample)\n\n    text = sample[\"essay_text\"][0]\n    chunks = []\n    prev = 0\n\n    zipped = zip(\n        sample[\"idxs\"],\n        sample[\"discourse_type\"],\n    )\n    for idxs, disc_type in zipped:\n        # when the discourse_text wasn't found\n        if idxs == [-1]:\n            continue\n\n        s, e = idxs\n\n        # if the start of the current discourse_text is not\n        # at the end of the previous one.\n        # (text in between discourse_texts)\n        if s != prev:\n            chunks.append(text[prev:s])\n            prev = s\n\n        # if the start of the current discourse_text is\n        # the same as the end of the previous discourse_text\n        if s == prev:\n            chunks.append(cls_tokens_map[disc_type])\n            chunks.append(text[s:e])\n            chunks.append(end_tokens_map[disc_type])\n\n        prev = e\n\n    tokenized = tokenizer(\n        \" \".join(chunks),\n        padding=False,\n        truncation=True,\n        max_length=CFG.max_len,\n        add_special_tokens=True,\n    )\n\n    return tokenized\n\ntest_df = pd.read_csv(INPUT_DIR + \"test.csv\")\n\nessay_text_ds = datasets.Dataset.from_dict({\"essay_id\": test_df.essay_id.unique()})\nessay_text_ds = essay_text_ds.map(\n        partial(get_essay_text, data_dir=INPUT_DIR),\n        num_proc=1,\n        batched=False,\n        desc=\"Loading text files\",\n)\nessay_text_df = essay_text_ds.to_pandas()\n\ntest_df[\"discourse_text\"] = [resolve_encodings_and_normalize(x) for x in test_df[\"discourse_text\"]]\ntest_df = test_df.merge(essay_text_df, on=\"essay_id\", how=\"left\")\ndel essay_text_df\n\n# track the matchings of each discourse in its essay, by the order given in the csv file\n\ndiscourse_text_values = test_df['discourse_text'].values\nessay_text_values = test_df['essay_text'].values\n\nmatches = []\nfor i, dt in enumerate(discourse_text_values):\n    if dt.strip() in essay_text_values[i]:\n        matches.append(1)\n    else:\n        matches.append(0)\ntest_df['match'] = matches\n\ngrouped_df = test_df.groupby([\"essay_id\"]).agg(list)\n\nds = datasets.Dataset.from_pandas(grouped_df)\nds = ds.map(\n        tokenize,\n        batched=False,\n        num_proc=1,\n        desc=\"Tokenizing\",\n)\n\nbad_matches = []\ncls_ids = set(list(cls_id_map.values()))\nfor id_, ids, dt in zip(ds[\"essay_id\"], ds[\"input_ids\"], ds[\"discourse_id\"]):\n    # count number of cls ids\n    num_cls_id = sum([x in cls_ids for x in ids])\n    # true number of discourse_texts\n    num_dt = len(dt)\n\n    if num_cls_id != num_dt:\n        bad_matches.append((id_, ids, dt))\n\nprint(\"Num bad matches:\", len(bad_matches))\nprint()\n\ncollator_fn = DataCollatorForTokenClassification(\n    tokenizer=tokenizer, pad_to_multiple_of=8, padding=True\n)\n\nmodel_config = AutoConfig.from_pretrained(model_path)\n\npreds = []\n\nfor fold in range(CFG.n_fold):\n    if fold in CFG.trn_fold:\n        print(f\"fold = {fold}\")\n        \n        checkpoint = f'../input/feedback-debertav3large-token-cls-models/pytorch_model_2048_fold{fold}.bin'\n        model = AutoModelForTokenClassification.from_pretrained(checkpoint, config=model_config)\n        \n        keep_cols = {\"input_ids\", \"attention_mask\"}\n        test_dataset = ds.remove_columns([c for c in ds.column_names if c not in keep_cols])\n        \n        trainer = Trainer(\n            model=model,\n            tokenizer=tokenizer,\n            data_collator=collator_fn,\n        )\n        \n        pred = trainer.predict(test_dataset)\n        preds.append(pred[0])\n        \n        del model\n        gc.collect()\n        torch.cuda.empty_cache()\n\n# get predicted logits\n\npreds = np.array(preds)\nlogits = np.exp(preds) / np.expand_dims(np.sum(np.exp(preds), axis=-1), axis=-1)\n\nmodel_preds = np.mean(logits, axis=0)\n\n# extract the predicted labels for all [discourse_type_CLS] \n\nhead_preds = []\nfor i, sample in enumerate(model_preds):\n    sample_pred = []\n    sample_ids = ds['input_ids'][i]\n    for j, tk_id in enumerate(sample_ids):\n        if tk_id in cls_ids:\n            sample_pred.append(sample[j])\n    head_preds.append(sample_pred)\n\n# collect predictions of each discourse by the order given in the csv file\n\nessay_id_map = {v : k for k, v in enumerate(ds['essay_id'])}\n\nfinal_preds = []\n\nordered_essay_ids = test_df['essay_id'].values\ndisordered_essay_matches = grouped_df['match'].values\n\npre_essay_id = ''\nfor essay_id in ordered_essay_ids:\n    if essay_id == pre_essay_id:\n        continue\n    pre_essay_id = essay_id\n    essay_pred = head_preds[essay_id_map[essay_id]]\n    essay_macth = disordered_essay_matches[essay_id_map[essay_id]]\n    for i, discourse_match in enumerate(essay_macth):\n        if discourse_match == 1 and i < len(essay_pred):\n            final_preds.append(essay_pred[i])\n        else:\n            final_preds.append([0., 0., 0.])\n            \nfinal_preds = np.array(final_preds)\n\npreds_Ineffective = final_preds[:, 2]\npreds_Adequate = final_preds[:, 0]\npreds_Effective = final_preds[:, 1]\n\nsample = pd.read_csv(INPUT_DIR + 'sample_submission.csv')\n\nsample['Ineffective'] = preds_Ineffective\nsample['Adequate'] = preds_Adequate\nsample['Effective'] = preds_Effective\n\n\nsample.to_csv('sub4.csv', index=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-19T16:56:34.716589Z","iopub.execute_input":"2022-08-19T16:56:34.717222Z","iopub.status.idle":"2022-08-19T16:56:34.733471Z","shell.execute_reply.started":"2022-08-19T16:56:34.717184Z","shell.execute_reply":"2022-08-19T16:56:34.732387Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!python token2.py","metadata":{"execution":{"iopub.status.busy":"2022-08-19T16:58:45.607533Z","iopub.execute_input":"2022-08-19T16:58:45.607920Z","iopub.status.idle":"2022-08-19T17:00:53.810433Z","shell.execute_reply.started":"2022-08-19T16:58:45.607869Z","shell.execute_reply":"2022-08-19T17:00:53.809002Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### 集成","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nsub1 = pd.read_csv('sub1.csv').sort_values(['discourse_id']).reset_index(drop=True)\nsub2 = pd.read_csv('sub2.csv').sort_values(['discourse_id']).reset_index(drop=True)\nsub3 = pd.read_csv('sub3.csv').sort_values(['discourse_id']).reset_index(drop=True)\nsub4 = pd.read_csv('sub4.csv').sort_values(['discourse_id']).reset_index(drop=True)\nsub1['Ineffective'] = sub1['Ineffective']*0.25 + sub2['Ineffective']*0.25 + sub3['Ineffective']*0.25 + sub4['Ineffective']*0.25\n\nsub1['Adequate'] = sub1['Adequate']*0.25 + sub2['Adequate']*0.2 + sub3['Adequate']*0.3 + sub4['Adequate']*0.25\n\nsub1['Effective'] = sub1['Effective']*0.25 + sub2['Effective']*0.2 + sub3['Effective']*0.3 + sub4['Effective']*0.25\nsub1.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}